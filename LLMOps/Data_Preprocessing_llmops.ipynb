{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# L2: Data Exploration for Tuning a Foundation Model"
      ],
      "metadata": {
        "id": "WsHIxXOEr6gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project environment setup:**\n",
        "\n",
        "- Load credentials and relevant Python Libraries\n",
        "- If you were running this notebook locally, you would first install Vertex AI. In this classroom, this is already installed.\n",
        "\n",
        "```\n",
        "!pip install google-cloud-aiplatform\n",
        "```\n",
        "- You can download the `requirements.txt` for this course from the workspace of this lab. `File --> Open...`"
      ],
      "metadata": {
        "id": "WuXZ437Fr6dg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5XXsmDlr41B"
      },
      "outputs": [],
      "source": [
        "from utils import authenticate\n",
        "credentials, PROJECT_ID = authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = \"us-central1\""
      ],
      "metadata": {
        "id": "HW2pU35OsB_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Import the [Vertex AI](https://cloud.google.com/vertex-ai) SDK.\n",
        "- The library helps to interact with the Vertex AI services in the cloud.\n",
        "- Initialize it."
      ],
      "metadata": {
        "id": "4n2bjbBRsKj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai"
      ],
      "metadata": {
        "id": "Uqbo9Ya6sCD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vertexai.init(project = PROJECT_ID,\n",
        "              location = REGION,\n",
        "              credentials = credentials)"
      ],
      "metadata": {
        "id": "4tCw5F1KsCF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Import [BigQuery](https://cloud.google.com/bigquery) to use as your data warehouse.\n",
        "- Initialize the client to start interacting with the data warehouse, send SQL and retrieve data into the notebook."
      ],
      "metadata": {
        "id": "tyKLbi1MsTU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery"
      ],
      "metadata": {
        "id": "hKVGqI42sCIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bq_client = bigquery.Client(project=PROJECT_ID,\n",
        "                            credentials = credentials)"
      ],
      "metadata": {
        "id": "vF-BZyEJsCK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- You will use [Stack Overflow Data](https://cloud.google.com/blog/topics/public-datasets/google-bigquery-public-datasets-now-include-stack-overflow-q-a) on BigQuery Public Datasets.\n",
        "- The datasets include questions, answers and metadata related to Stack Overflow questions. Within this dataset, there are tables with data.\n",
        "- Create a SQL query."
      ],
      "metadata": {
        "id": "2CONWfeQsbvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY_TABLES = \"\"\"\n",
        "SELECT\n",
        "  table_name\n",
        "FROM\n",
        "  `bigquery-public-data.stackoverflow.INFORMATION_SCHEMA.TABLES`\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SsTZAzdssCNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The query is asking to retrieve `table_name` of all the `TABLES`\n",
        "- Use the client to send your SQL and retrieve the data (tables names)."
      ],
      "metadata": {
        "id": "EDoa0Q4-siHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_job = bq_client.query(QUERY_TABLES)"
      ],
      "metadata": {
        "id": "yQuGn36OsCPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in query_job:\n",
        "    for value in row.values():\n",
        "        print(value)"
      ],
      "metadata": {
        "id": "_65I10wQsCRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Retrieval"
      ],
      "metadata": {
        "id": "8A5vTNxasqDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- You'll fetch some data from the data warehouse and store it in Pandas dataframe for visualization.\n",
        "- Select all columns from  `posts_questions` and put the `LIMIT` as 3."
      ],
      "metadata": {
        "id": "8ladxWHNsp-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INSPECT_QUERY = \"\"\"\n",
        "SELECT\n",
        "    *\n",
        "FROM\n",
        "    `bigquery-public-data.stackoverflow.posts_questions`\n",
        "LIMIT 3\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "u7zQh7PRsCUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "6x4vunA6sCWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_job = bq_client.query(INSPECT_QUERY)"
      ],
      "metadata": {
        "id": "toI1C8ycsCYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Take the results of the query `-->` create an arrow table (which is part of [Apache Framework](https://arrow.apache.org/docs/index.html)) `-->` which goes into a Pandas dataframe.\n",
        "- This allows for data to be in a format which is easier to read and explore with Pandas."
      ],
      "metadata": {
        "id": "Iu2UrTwIs4QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stack_overflow_df = query_job\\\n",
        "    .result()\\\n",
        "    .to_arrow()\\\n",
        "    .to_pandas()\n",
        "stack_overflow_df.head()"
      ],
      "metadata": {
        "id": "0ggadabdsCa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with Large Datasets\n",
        "\n",
        "- Large datasets for LLMs often don't fit into memory.\n",
        "- Select all of the columns and rows of the table `posts_questions`."
      ],
      "metadata": {
        "id": "ppSylfp2tGOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY_ALL = \"\"\"\n",
        "SELECT\n",
        "    *\n",
        "FROM\n",
        "    `bigquery-public-data.stackoverflow.posts_questions` q\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9NgiLJFYsCdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_job = bq_client.query(QUERY_ALL)"
      ],
      "metadata": {
        "id": "Tv7-YSkftJ3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    stack_overflow_df = query_job\\\n",
        "    .result()\\\n",
        "    .to_arrow()\\\n",
        "    .to_pandas()\n",
        "except Exception as e:\n",
        "    print('The DataFrame is too large to load into memory.', e)"
      ],
      "metadata": {
        "id": "LTigXsv7tJ7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The data is too large to return, as it is not fitting into memory."
      ],
      "metadata": {
        "id": "l1Pr92x2tfP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Joining Tables and Query Optimization\n",
        "\n",
        "- When working with (large) data, query optimizing is needed in order to save time and resources.\n",
        "- Select questions as `input_text` (column 1), answers as `output_text` (column 2).\n",
        "- Take the questions from `posts_questions` and answers from `posts_answers`.\n",
        "- Join the questions and their corresponding accepted answers based on their same `unique ID`.\n",
        "- Making sure the question is about `Python`, and that it `has an answer`. And the date the question was posted is on or after `2020-01-01`\n",
        "- Limit as 10,000"
      ],
      "metadata": {
        "id": "uxzH0TnVtZsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = \"\"\"\n",
        "SELECT\n",
        "    CONCAT(q.title, q.body) as input_text,\n",
        "    a.body AS output_text\n",
        "FROM\n",
        "    `bigquery-public-data.stackoverflow.posts_questions` q\n",
        "JOIN\n",
        "    `bigquery-public-data.stackoverflow.posts_answers` a\n",
        "ON\n",
        "    q.accepted_answer_id = a.id\n",
        "WHERE\n",
        "    q.accepted_answer_id IS NOT NULL AND\n",
        "    REGEXP_CONTAINS(q.tags, \"python\") AND\n",
        "    a.creation_date >= \"2020-01-01\"\n",
        "LIMIT\n",
        "    10000\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_W6IRjBotJ-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_job = bq_client.query(QUERY)"
      ],
      "metadata": {
        "id": "fVy3tfU6tKAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### this may take some seconds to run\n",
        "stack_overflow_df = query_job.result()\\\n",
        "                        .to_arrow()\\\n",
        "                        .to_pandas()\n",
        "\n",
        "stack_overflow_df.head(2)"
      ],
      "metadata": {
        "id": "6bvvdLz9tKCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Instructions\n",
        "\n",
        "- Instructions for LLMs have been shown to improve\n",
        "model performance and generalization to unseen tasks [(Google, 2022)](https://arxiv.org/pdf/2210.11416.pdf).\n",
        "- Wihtout the instruction, it is only question and answer. Model might not understand what to do.\n",
        "- With the instructions, the model gets a guideline as to what task to perform."
      ],
      "metadata": {
        "id": "j-MNdg9xtr_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INSTRUCTION_TEMPLATE = f\"\"\"\\\n",
        "Please answer the following Stackoverflow question on Python. \\\n",
        "Answer it like you are a developer answering Stackoverflow questions.\n",
        "\n",
        "Stackoverflow question:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "siDAbf1ltKER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A new column will combine `INSTRUCTION_TEMPLATE` and the question `input_text`.\n",
        "- This avoids overwritting of any existing column which might be needed."
      ],
      "metadata": {
        "id": "zIE08A1HtyYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stack_overflow_df['input_text_instruct'] = INSTRUCTION_TEMPLATE + ' '\\\n",
        "    + stack_overflow_df['input_text']"
      ],
      "metadata": {
        "id": "7ks3nuOHtKGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset for Tuning\n",
        "\n",
        "- Divide the data into a training and evaluation. By default, 80/20 split is used.\n",
        "- This (80/20 split) allows for more data to be used for tuning. The evaluation split is used as unseen data during tuning to evaluate performance.\n",
        "- The `random_state` parameter is used to ensure random sampling for a fair comparison."
      ],
      "metadata": {
        "id": "fqr7kEqmt5EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "C-kA5pkftKIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, evaluation = train_test_split(\n",
        "    stack_overflow_df,\n",
        "    ### test_size=0.2 means 20% for evaluation\n",
        "    ### which then makes train set to be of 80%\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "6cZGu9j9tKLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Different Datasets and Flow\n",
        "\n",
        "- Versioning data is important.\n",
        "- It allows for reproducibility, traceability, and maintainability of machine learning models.\n",
        "- Get the timestamp."
      ],
      "metadata": {
        "id": "lUdsS5dvuCoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "K0M6ix5wtKXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")"
      ],
      "metadata": {
        "id": "S7WC-La-uHTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Generate a `jsonl` file.\n",
        "- Name it as `tune_data_stack_overflow_python_qa-{date}`"
      ],
      "metadata": {
        "id": "G5IxGL7duMd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['input_text_instruct','output_text']\n",
        "tune_jsonl = train[cols].to_json(orient=\"records\", lines=True)"
      ],
      "metadata": {
        "id": "z2cHXw_AuHXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_filename = f\"tune_data_stack_overflow_\\\n",
        "                            python_qa-{date}.jsonl\""
      ],
      "metadata": {
        "id": "tjw8HxoGtKbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(training_data_filename, \"w\") as f:\n",
        "    f.write(tune_jsonl)"
      ],
      "metadata": {
        "id": "yGVdZ1I2tKfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}